
<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8" />
        <title>利用ChatGLM利用QQ数据集构建自己的数字孪生 | LinkedList&#39;s Blog</title>
        <meta name="author" content="Ding Li" />
        <meta name="description" content="This is blog for Linkedlist771, mainly record some learning notes, thank you for your visit!" />
        <meta name="keywords" content="" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
        <link rel="icon" href="/images/favicon.png" />
        <script src="https://cdn.staticfile.org/vue/3.2.47/vue.global.prod.min.js"></script>
<link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/6.3.0/css/all.min.css" />
<link rel="stylesheet" href="/css/fonts.min.css" />
<script> const mixins = {}; </script>

<script src="https://polyfill.io/v3/polyfill.min.js?features=default"></script>


<script src="https://cdn.staticfile.org/highlight.js/11.7.0/highlight.min.js"></script>
<link
    rel="stylesheet"
    href="https://cdn.staticfile.org/highlight.js/11.7.0/styles/github.min.css"
/>
<script src="/js/lib/highlight.js"></script>


<script src="https://cdn.staticfile.org/KaTeX/0.16.4/katex.min.js"></script>
<script src="https://cdn.staticfile.org/KaTeX/0.16.4/contrib/auto-render.min.js"></script>
<link rel="stylesheet" href="https://cdn.staticfile.org/KaTeX/0.16.4/katex.min.css" />
<script src="/js/lib/math.js"></script>


<script src="/js/lib/preview.js"></script>









<link rel="stylesheet" href="/css/main.css" />

    <meta name="generator" content="Hexo 6.3.0"></head>
    <body>
        <div id="layout">
            <transition name="fade">
                <div id="loading" v-show="loading">
                    <div id="loading-circle">
                        <h2>LOADING</h2>
                        <p>加载过慢请开启缓存 浏览器默认开启</p>
                        <img src="/images/loading.gif" />
                    </div>
                </div>
            </transition>
            <nav id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}">
    <div id="desktop-menu">
        <a class="title" href="/">
            <span>LINKEDLIST&#39;S BLOG</span>
        </a>
        
        <a href="/">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;Home</span>
        </a>
        
        <a href="/about">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;About</span>
        </a>
        
        <a href="/archives">
            <i class="fa-solid fa-box-archive fa-fw"></i>
            <span>&ensp;Archives</span>
        </a>
        
        <a href="/categories">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;Categories</span>
        </a>
        
        <a href="/tags">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;Tags</span>
        </a>
        
    </div>
    <div id="mobile-menu">
        <div class="title" @click="showMenuItems = !showMenuItems">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;LINKEDLIST&#39;S BLOG</span>
        </div>
        <transition name="slide">
            <div class="items" v-show="showMenuItems">
                
                <a href="/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-house fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Home</div>
                    </div>
                </a>
                
                <a href="/about">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-id-card fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">About</div>
                    </div>
                </a>
                
                <a href="/archives">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-box-archive fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Archives</div>
                    </div>
                </a>
                
                <a href="/categories">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-bookmark fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Categories</div>
                    </div>
                </a>
                
                <a href="/tags">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-tags fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Tags</div>
                    </div>
                </a>
                
            </div>
        </transition>
    </div>
</nav>
<transition name="fade">
    <div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div>
</transition>

            <transition name="into">
                <div id="main" v-show="!loading">
                    <div class="article">
    <div>
        <h1>利用ChatGLM利用QQ数据集构建自己的数字孪生</h1>
    </div>
    <div class="info">
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2023/5/30
        </span>
        
        
        <span class="tags">
            <span class="icon">
                <i class="fa-solid fa-tags fa-fw"></i>
            </span>
            
            <span class="tag">
                
                <a href="/tags/ChatGLM%EF%BC%8C-%E6%95%B0%E5%AD%97%E5%AD%AA%E7%94%9F%EF%BC%8C-QQ%E6%95%B0%E6%8D%AE%E9%9B%86/" style="color: #03a9f4">ChatGLM， 数字孪生， QQ数据集</a>
            </span>
            
        </span>
        
    </div>
    
    <div class="content" v-pre>
        <h3 id="项目简介："><a href="#项目简介：" class="headerlink" title="项目简介："></a>项目简介：</h3><p>本项目源自<a target="_blank" rel="noopener" href="https://github.com/kcxain/CloneLLM">kcxain</a>, 本人只是将其在本地部署完成， 作者使用了<code>NVIDIA A100 80GB</code> 并不是每个人都拥有的。也不是每一个人都拥有<code>Linux</code>系统进行部署，并且对于<code>ChatGLM</code>的部署，作者没有说的很明白。所以这个项目将分为两个部分：</p>
<ol>
<li><code>ChatGLM</code>的部署</li>
<li>基于该项目的自我数据集的训练</li>
</ol>
<h3 id="ChatGLM部署"><a href="#ChatGLM部署" class="headerlink" title="ChatGLM部署"></a>ChatGLM部署</h3><ul>
<li>ChatGLM-6B 是一个开源的、支持中英双语的对话语言模型，基于 General Language Model (GLM) 架构，具有 62 亿参数<a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM-6B">1</a>。</li>
<li>结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）<a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM-6B">1</a>。</li>
<li>ChatGLM-6B 使用了和 ChatGPT 相似的技术，针对中文问答和对话进行了优化<a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM-6B">1</a>。</li>
<li>经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答<a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM-6B">1</a>。</li>
<li>为了方便下游开发者针对自己的应用场景定制模型，他们同时实现了基于 P-Tuning v2 的高效参数微调方法 (使用指南) ，INT4 量化级别下最低只需 7GB 显存即可启动微调<a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM-6B">1</a>。</li>
<li>ChatGLM-6B 开源模型旨在与开源社区一起推动大模型技术发展，恳请开发者和大家遵守开源协议，勿将开源模型和代码及基于开源项目产生的衍生物用于任何可能给国家和社会带来危害的用途以及用于任何未经过安全评估和备案的服务<a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM-6B">1</a>。</li>
</ul>
<p>清华的 ChatGLM 项目在 2023 年也发布了一些重要的更新：</p>
<ul>
<li>2023 年 5 月 17 日，他们发布了 VisualGLM-6B，一个支持图像理解的多模态对话语言模型<a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM-6B">1</a>。</li>
<li>2023 年 5 月 15 日，他们更新了 v1.1 版本 checkpoint，训练数据增加英文指令微调数据以平衡中英文数据比例，解决英文回答中夹杂中文词语的现象<a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM-6B">1</a>。</li>
</ul>
<h4 id="部署ChatGLM"><a href="#部署ChatGLM" class="headerlink" title="部署ChatGLM"></a>部署ChatGLM</h4><ol>
<li><p>进入<a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM-6B">ChatGLM</a> 主页，找到git链接，git到本地即可:</p>
<p><img src="/../images/image-20230530173507682.png" alt="image-20230530173507682"></p>
<p>然后使用git 命令下载到本地即可：</p>
<pre><code class="bash">git clone git@github.com:THUDM/ChatGLM-6B.git
</code></pre>
<p>如果不熟悉git， 也可以点击主页的下载按钮下载zip文件进行解压。</p>
<p><img src="/../images/image-20230530173631415.png" alt="image-20230530173631415"></p>
</li>
<li><p>下载完ChatGLM主页程序后，现在需要下载模型的权重文件。进入ChatGLM的<a target="_blank" rel="noopener" href="https://huggingface.co/THUDM/chatglm-6b">hugging face hub</a></p>
<p>利用按照主页教程利用git lsf 进行下载即可。 </p>
<p>如果这一步也存在问题，也可以先下载项目文件， 将权重文件下载并保存这个文件里面的其他的文件到chatglm-6b文件夹（<code>注意，该文件夹要在之前的ChatGLM文件夹下</code>）：</p>
<p><img src="/../images/image-20230530174240127.png" alt="image-20230530174240127"></p>
</li>
<li><p>下载完毕后，我们便可以试着运行一下预训练的<code>ChatGLM</code>了， 注意，这里根据你电脑的显存不同采用的量化规则也不太相同， 官网的建议如下：</p>
</li>
</ol>
<table>
<thead>
<tr>
<th><strong>量化等级</strong></th>
<th><strong>最低 GPU 显存</strong>（推理）</th>
<th><strong>最低 GPU 显存</strong>（高效参数微调）</th>
</tr>
</thead>
<tbody><tr>
<td>FP16（无量化）</td>
<td>13 GB</td>
<td>14 GB</td>
</tr>
<tr>
<td>INT8</td>
<td>8 GB</td>
<td>9 GB</td>
</tr>
<tr>
<td>INT4</td>
<td>6 GB</td>
<td>7 GB</td>
</tr>
</tbody></table>
<p>​       如果你的电脑是8G显存， 本人实测也只能选择<code>INT4</code> 量化， 对于6G以下显存的用户不太友好，你这里可以尝试<a target="_blank" rel="noopener" href="https://github.com/Jittor/JittorLLMs">清华的另一个项目</a> , 基于<code>动态计算图</code>的<code>硬盘，内存，显存</code>动态交换量化的方式进行运行。笔者的显卡为<code>NVIDIA RTX 3070Ti-Laptop</code>， 显存为8G，选择为<code>INT4</code>量化。</p>
<p>​       现在我们已经完成了所有的预设步骤啦，现在可以部署实验了， 选择这个文件：</p>
<p><img src="/../images/image-20230530175109292.png" alt="image-20230530175109292"></p>
<p>将第7-8行的代码修改为：</p>
<p><code>INT4量化</code>：</p>
<pre><code class="python">tokenizer = AutoTokenizer.from_pretrained(&quot;chatglm-6b&quot;, trust_remote_code=True)
model = AutoModel.from_pretrained(&quot;chatglm-6b&quot;, trust_remote_code=True).quantize(4).half().cuda()
</code></pre>
<p>或者<code>INT8量化</code>:</p>
<pre><code class="python">tokenizer = AutoTokenizer.from_pretrained(&quot;chatglm-6b&quot;, trust_remote_code=True)
model = AutoModel.from_pretrained(&quot;chatglm-6b&quot;, trust_remote_code=True).quantize(8).half().cuda()
</code></pre>
<p>然后进行运行即可，现在我们就可以给他提问了，取决于你的显卡的显存和算力，回复速度可能存在差距。</p>
<p><img src="/../images/image-20230530183000569.png" alt="image-20230530183000569"></p>
<p><img src="/../images/image-20230530183459075.png" alt="image-20230530183459075"></p>
<h4 id="基于自己的QQ数据集的P-tuning"><a href="#基于自己的QQ数据集的P-tuning" class="headerlink" title="基于自己的QQ数据集的P-tuning"></a>基于自己的QQ数据集的P-tuning</h4><p>参考<a target="_blank" rel="noopener" href="https://github.com/kcxain/CloneLLM">kcxain</a>,的介绍，你现在应该能够完成数据集的构建，然后我们就可以开始训练了， 这里需要注意的是，作者使用的是<code>Linux</code>系统编写的训练脚本，所以我们需要在<code>windows</code>下进行训练需要进行修改。</p>
<p>这里我们选择<code>P-tuning</code>下面的<code>main.py</code>文件</p>
<p><img src="/../images/image-20230530175722861.png" alt="image-20230530175722861"></p>
<p>在pycharm中我们可以在这里设置参数</p>
<p><img src="/../images/image-20230530175742124.png" alt="image-20230530175742124"></p>
<p>在这里输入参数：</p>
<p><img src="/../images/image-20230530175804152.png" alt="image-20230530175804152"></p>
<p>我这里选择的参数为：</p>
<pre><code class="bash">--quantization_bit
4
--do_train
--train_file
我的好友.txt.json
--prompt_column
prompt
--response_column
response
--history_column
history
--overwrite_cache
--model_name_or_path
..\chatglm-6b
--output_dir
chatglm_qq
--overwrite_output_dir
--max_source_length
128
--max_target_length
128
--per_device_train_batch_size
4
--per_device_eval_batch_size
1
--gradient_accumulation_steps
2
--predict_with_generate
--max_steps
10000
--logging_steps
10
--save_steps
1000
--learning_rate
0.002
--pre_seq_len
128
</code></pre>
<p>这里需要说明一下，根据你的显卡的显存大小，可以适当调整<code>batch_size</code>,<code>max_source_length</code>,<code>max_target_length  </code>从而减小你的显存消耗，这里我设置的参数的显存占用为<code>7.8/8G</code>, 基本上吃满了。</p>
<p><img src="/../images/image-20230530180019282.png" alt="image-20230530180019282"></p>
<p>然后我们运行后便可以看到训练的log输出了。</p>
<p><img src="/../images/image-20230530180052355.png" alt="image-20230530180052355"></p>
<p>这里我设置的每隔1000个epoch进行一个保存，在训练1000个epoch后，我终止了。这是目前的对话效果：</p>
<p>按照博主的介绍，这里我们注意一下，传入的参数是在py程序中</p>
<pre><code class="BASH">--model_name_or_path
..\chatglm-6b
--pre_seq_len
128
--ptuning_checkpoint
chatglm_qq/checkpoint-1000
</code></pre>
<p><img src="/../images/image-20230530185103745.png" alt="image-20230530185103745"></p>
<p>好吧，看来我和QQ群友主要的讨论内容还是关于文件接受&#x2F;传输的， 太尴尬了，后面准备拿QQ群聊数据训练一下 ，毕竟我的活跃主要是在QQ群聊立马🤣。 </p>
<p>笔者又进行了训练，得到如下效果：</p>
<p><img src="/../images/image-20230530201939083.png" alt="image-20230530201939083"></p>
<p>怎么说呢，差强人意吧， 可能还是预料数据集质量较差。</p>
<h3 id="构建自己的数字孪生"><a href="#构建自己的数字孪生" class="headerlink" title="构建自己的数字孪生"></a>构建自己的数字孪生</h3><p>在前文中，我们完成了基于自己的QQ数据集的<code>P-tuning</code>得到的自己的对话模型，现在我们可以部署了，这里我采用的是一个常用的解决方案<code>cqhttp+nonebot</code>, 然后选择基于已有插件进行修改得到。<code>nonebot</code>是一个基于异步架构搭建的， 基于此，我们基于<code>fast api</code>搭建一个异步路由构建自己的数字孪生。借助<code>GPT4</code>，简单描述需求后便可以得到所需要的代码：</p>
<pre><code class="python">from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import (
    AutoConfig,
    AutoModel,
    AutoTokenizer,
)
from arguments import ModelArguments, DataTrainingArguments
import torch
import os

app = FastAPI()

model = None
tokenizer = None


class Prompt(BaseModel):
    text: str

@app.on_event(&quot;startup&quot;)
async def load_model():
    global model, tokenizer

    model_args = ModelArguments(
        model_name_or_path=&quot;..\\chatglm-6b&quot;,
        ptuning_checkpoint=&quot;chatglm_qq/checkpoint-600&quot;,
        pre_seq_len=128,
        prefix_projection=False,
        quantization_bit=8,
    )

    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path,
                                              trust_remote_code=True)
    config = AutoConfig.from_pretrained(model_args.model_name_or_path,
                                        trust_remote_code=True)

    config.pre_seq_len = model_args.pre_seq_len
    config.prefix_projection = model_args.prefix_projection

    if model_args.ptuning_checkpoint is not None:
        print(
            f&quot;Loading prefix_encoder weight from &#123;model_args.ptuning_checkpoint&#125;&quot;
        )
        model = AutoModel.from_pretrained(model_args.model_name_or_path,
                                          config=config,
                                          trust_remote_code=True).quantize(4).half().cuda()
        prefix_state_dict = torch.load(
            os.path.join(model_args.ptuning_checkpoint, &quot;pytorch_model.bin&quot;))
        new_prefix_state_dict = &#123;&#125;
        for k, v in prefix_state_dict.items():
            if k.startswith(&quot;transformer.prefix_encoder.&quot;):
                new_prefix_state_dict[
                    k[len(&quot;transformer.prefix_encoder.&quot;):]] = v
        model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)
    else:
        model = AutoModel.from_pretrained(model_args.model_name_or_path,
                                          config=config,
                                          trust_remote_code=True)

    if model_args.quantization_bit is not None:
        print(f&quot;Quantized to &#123;model_args.quantization_bit&#125; bit&quot;)
        model = model.quantize(model_args.quantization_bit)

    if model_args.pre_seq_len is not None:
        # P-tuning v2
        model = model.half().cuda()
        model.transformer.prefix_encoder.float().cuda()

    model = model.eval()
    print(f&quot;模型加载成功&quot;)


@app.post(&quot;/chatglm/qq/ask&quot;)
async def ask(prompt: Prompt):
    global model, tokenizer
    if model is None or tokenizer is None:
        raise HTTPException(status_code=503, detail=&quot;Model not loaded&quot;)

    # 现在你需要使用model和tokenizer处理prompt并生成response
    # response = model.chat(prompt)
    # 调用模型的 chat 方法
    history = []
    response, history = model.chat(
        tokenizer,
        prompt.text,
        history=history,
    )

    return &#123;&quot;response&quot;: response&#125;



async def main():
    await load_model()
    while True:
        prompt = input(&quot;请输入：&quot;)
        response = await ask(Prompt(text=prompt))
        print(response)


if __name__ == &quot;__main__&quot;:
    import asyncio
    asyncio.run(main())
</code></pre>
<p>启动代码为：</p>
<pre><code class="bash"> uvicorn qq_chat_app:app --host 0.0.0.0 --port 1414 --reload
</code></pre>
<p>简单编写测试代码测试端口：</p>
<pre><code class="python">import requests
def get_chatglm(prompt):
    url = &#39;http://localhost:1414/chatglm/qq/ask&#39;
    data = &#123;
        &#39;text&#39;: prompt
    &#125;
    response = requests.post(url, json=data)

    print(response.json()[&quot;response&quot;])

get_chatglm(&quot;你是谁？ 你能干什么？ 今天晚上什么吃什么？&quot;)
</code></pre>
<p>得到结果:</p>
<pre><code class="bash">我是一个名为 ChatGPT 的人工智能助手，我可以通过自然语言处理技术，回答您的问题和提供信息。
</code></pre>
<p>成功测试通接口，暂时基于<code>nonebot2</code>搭建了一个简单的回复框架，效果如下：</p>
<p><img src="/../images/image-20230530214355521.png" alt="image-20230530214355521"></p>
<p><img src="/../images/image-20230530214405417.png" alt="image-20230530214405417"></p>
<p><img src="/../images/image-20230530214524635.png" alt="image-20230530214524635"></p>
<p>由于目前<code>tx</code>对QQ机器人风控严重，回复了几次就不行了，而且回复效果只能算一般吧。</p>
<h1 id="Todo-List"><a href="#Todo-List" class="headerlink" title="Todo List"></a>Todo List</h1><ul>
<li><input disabled type="checkbox"> 优化数据集构建， 提高对话质量，增强模型训练。</li>
<li><input disabled type="checkbox"> 尝试注入<code>RWKV</code>, <code>LLM</code>等模型进行构建。</li>
<li><input disabled type="checkbox"> 暂时还没想好。</li>
</ul>

    </div>
    
    
    
    
    
    
    
</div>

                    <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2022 - 2023 LinkedList&#39;s Blog
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;Ding Li
        </div>
        <div>
            Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp;
            <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a>
        </div>
        
    </div>


    
        <!-- 不蒜子统计 -->
        <span id="busuanzi_container_site_pv">
                本站总访问量<span id="busuanzi_value_site_pv"></span>次
        </span>
        <span class="post-meta-divider">|</span>
        <span id="busuanzi_container_site_uv" style='display:none'>
                本站访客数<span id="busuanzi_value_site_uv"></span>人
        </span>
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
</footer>

                </div>
            </transition>
            
            <transition name="fade">
                <div id="preview" ref="preview" v-show="previewShow">
                    <img id="preview-content" ref="previewContent" />
                </div>
            </transition>
            
        </div>
        <script src="/js/main.js"></script>
        
        




        
    </body>
</html>
